ML Hackathon - Analysis Report
TEAM NUMBER:16
CHANDAN R  [PES1UG23AM917]
SRUJAN J  [PES1UG24AM814]
CHAKRESH [PES1UG23AM347]
1. Key Observations

Challenges:

• State Design: Encoding Hangman’s dynamic state into a 54D vector (26 HMM scores + 26 guessed + 2 stats) while keeping it compact and informative was complex.

• Reward Engineering: Balancing immediate vs. terminal rewards was crucial — combining (+10 correct, -5 wrong, -2 repeat, +15 win bonus) gave better convergence stability.

• Model Integration: Combining the probabilistic HMM predictions with DQN learning required consistent normalization and careful reward shaping.


Insights:

• The DQN agent learned a preference-based exploration policy, shifting from exploration (ε=1.0) to exploitation (ε=0.1) gradually.

• Replay buffer (100K) and target updates every 200 episodes improved Q-value stability.

• The model outperformed the baseline heuristic in consistency, though raw win rate was still moderate.

2. Strategies

HMM Design:
A bigram-based Hidden Markov Model was trained on the vocabulary (15 words) using Laplace smoothing (α=0.5). The model captured letter-to-letter transitions, enabling posterior scoring for masked word positions.

RL State & Reward:
The 54D state vector encoded letter probabilities, guessed letters, blanks left, and lives remaining. The reward system combined immediate feedback (+10/–5) and terminal incentives (+15 for win, –10 for loss) to promote efficient gameplay.

3. Exploration

An ε-greedy exploration policy was used:
• Early phase: High exploration (ε=1.0)
• Mid-phase: Balanced (ε≈0.5)
• Late phase: Focused exploitation (ε→0.1)

Replay buffer ensured diverse sampling, preventing catastrophic forgetting and helping stable convergence.

4. Results

Evaluation was performed on 2000 Hangman games using 10 common English words.

Results Summary:
• RL win rate: ~49.70%
• Wrong guesses: ~1006
• Repeated guesses: 0
• Final computed score: 8934.0
The results indicate partial learning success — improved policies emerge with more episodes or prioritized replay.

5. Future Improvements

• Use dueling or double DQN for more stable Q-value estimation.
• Add character embeddings for better state representation.
• Expand vocabulary and train on a larger corpus for stronger generalization.
• Integrate LSTM/Transformer-based predictors for hybrid symbolic+neural reasoning.
• Explore adaptive reward tuning for faster convergence.

Conclusion
This project successfully implemented a Reinforcement Learning-based Hangman solver using DQN integrated with an HMM-based letter predictor. The system demonstrates adaptive guessing behavior and generalization ability across unseen words. Although success rate remains below heuristic baselines, the framework provides a strong foundation for hybrid AI models combining probabilistic reasoning with deep RL.

