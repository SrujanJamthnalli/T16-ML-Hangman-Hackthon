Hangman Reinforcement Learning - Project README
ğŸ‘¥ Authors

Developed as part of ML Hackathon coursework

Team Members:
- PES1UG23AM917 - CHANDAN R
-PES1UG24AM814 â€“ SRUJAN J
-PES1UG23AM347 - CHAKRESH

ğŸ“‹ Project Overview

This project implements an intelligent Hangman game solver using a combination of a Hidden Markov Model (HMM) for probabilistic letter predictions and a Deep Q-Network (DQN) for decision-making. 
The goal is to create an agent that plays Hangman efficiently, maximizing success rate while minimizing wrong and repeated guesses.

ğŸ¯ Objective

Build an RL agent that can learn optimal guessing behavior through interaction with the Hangman environment, leveraging letter transition probabilities derived from the HMM model.

ğŸ“ Project Structure

â”œâ”€â”€ hmm_model.py       # Builds and trains the HMM for letter probability estimation
â”œâ”€â”€ train.py           # DQN training loop with replay buffer and target network
â”œâ”€â”€ stimulate.py       # Simulates a single Hangman game using trained model
â”œâ”€â”€ evaluate.py        # Evaluates the trained DQN agent on test words
â”œâ”€â”€ requirements.txt   # All required Python dependencies
â””â”€â”€ dqn_hangman_model.pth  # Saved trained model weights

âš™ï¸ Installation

1. Clone or download the repository containing all source files.
2. Install dependencies using:

â€¢	pip install -r requirements.txt

3. Ensure Python â‰¥ 3.9 and PyTorch â‰¥ 2.0 are installed.

ğŸš€ Usage

To train the model from scratch:

â€¢	python train.py

To test the trained agent on evaluation words:

â€¢	python evaluate.py

To visualize step-by-step predictions for a specific word:

â€¢	python stimulate.py
ğŸ§  Technical Details

â€¢ **State Representation (54D)**: Combination of 26 HMM posterior probabilities, 26 binary guessed indicators, and 2 normalized scalars (blanks_left, lives_left).

â€¢ **HMM (hmm_model.py)**: 
  - Trains a bigram transition model using Laplace smoothing.
  - Computes posterior probabilities for letter positions given masked patterns.

â€¢ **DQN (train.py)**: 
  - Uses replay buffer and target network for stability.
  - Employs Îµ-greedy policy with decaying Îµ for exploration-exploitation tradeoff.
  - Reward shaping encourages correct predictions and penalizes repetition.

â€¢ **Evaluation (evaluate.py)**: 
  - Runs multiple games, logging success rate, wrong/repeated guesses, and final score.

ğŸ“Š Expected Results

â€¢ Success Rate: Target 45-65% after training ~2000 episodes.
â€¢ Wrong Guesses: < 3 per game on average.
â€¢ Repeated Guesses: Approaching 0.
â€¢ Training Time: ~30â€“40 minutes depending on system performance.

ğŸ”® Future Enhancements

â€¢ Use Double or Dueling DQN to improve value stability.
â€¢ Add word embeddings to encode semantic similarity.
â€¢ Train on a larger and more diverse corpus for generalization.
â€¢ Introduce LSTM-based sequential prediction for better letter context understanding.




